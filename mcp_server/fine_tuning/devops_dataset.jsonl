{"instruction": "Analyze this CI/CD pipeline failure and provide recommendations", "input": "Pipeline failed at deployment stage with error: 'kubectl apply failed: unable to recognize \"\": no matches for kind \"Deployment\" in version \"apps/v1beta1\"'", "output": "**Root Cause Analysis**: The pipeline is using deprecated Kubernetes API version `apps/v1beta1` for Deployment resources. This API version was removed in Kubernetes 1.16+.\n\n**Impact**: Complete deployment failure, blocking releases.\n\n**Solution**: Update Kubernetes manifests to use `apps/v1` API version.\n\n**Implementation Steps**:\n1. Update all Deployment manifests from `apiVersion: apps/v1beta1` to `apiVersion: apps/v1`\n2. Verify compatibility with cluster version\n3. Test in staging environment\n4. Update CI/CD pipeline validation\n\n**Prevention**: Implement API version validation in CI pipeline using tools like kubeval or kube-score.", "domain": "devops"}
{"instruction": "Analyze SLO violation and recommend actions", "input": "Service availability SLO violated: 99.5% actual vs 99.9% target over last 30 days. Error budget exhausted.", "output": "**SLO Analysis**: Service availability dropped to 99.5%, consuming 100% of error budget (0.4% vs 0.1% allowed).\n\n**Impact**: \n- Error budget exhausted for 30-day window\n- No room for planned changes or incidents\n- Risk of customer impact\n\n**Immediate Actions**:\n1. Freeze non-critical deployments\n2. Focus on reliability improvements\n3. Increase monitoring and alerting\n\n**Root Cause Investigation**:\n- Analyze incident patterns and frequency\n- Review deployment impact on availability\n- Assess infrastructure stability\n\n**Long-term Improvements**:\n1. Implement chaos engineering practices\n2. Improve deployment safety (canary, blue-green)\n3. Enhance monitoring and early detection\n4. Consider relaxing SLO if current target is too aggressive", "domain": "sre"}
{"instruction": "Design cloud migration strategy for on-premises application", "input": "Legacy Java monolith with Oracle database, 10,000 daily users, 99.5% uptime requirement, compliance with PCI DSS", "output": "**Migration Strategy: Lift-Shift-Modernize Approach**\n\n**Phase 1: Lift (3-6 months)**\n- Migrate to AWS EC2 with similar configuration\n- Use RDS for Oracle for database\n- Implement VPC with proper security groups\n- Set up CloudWatch monitoring\n\n**Phase 2: Shift (6-12 months)**\n- Implement auto-scaling groups\n- Add Application Load Balancer\n- Migrate to managed services (RDS, ElastiCache)\n- Implement backup and disaster recovery\n\n**Phase 3: Modernize (12-18 months)**\n- Containerize application with ECS/EKS\n- Break monolith into microservices\n- Implement CI/CD with CodePipeline\n- Add observability with X-Ray and CloudWatch\n\n**PCI DSS Compliance**:\n- Use AWS Config for compliance monitoring\n- Implement encryption at rest and in transit\n- Set up CloudTrail for audit logging\n- Regular vulnerability scanning\n\n**Cost Optimization**:\n- Reserved Instances for predictable workloads\n- Spot Instances for non-critical components\n- Implement cost monitoring and budgets", "domain": "cloud"}
{"instruction": "Design internal developer platform for microservices", "input": "Company has 50 developers, 20 microservices, Kubernetes cluster, wants to improve developer experience and reduce deployment complexity", "output": "**Internal Developer Platform Design**\n\n**Core Components**:\n1. **Service Catalog**: Template-based service creation with golden paths\n2. **Self-Service Portal**: Web UI for developers to deploy and manage services\n3. **CI/CD Platform**: GitOps-based deployment pipeline\n4. **Observability Stack**: Centralized logging, metrics, and tracing\n\n**Golden Path Implementation**:\n- Standardized service templates (REST API, gRPC, event-driven)\n- Pre-configured CI/CD pipelines\n- Built-in security scanning and compliance checks\n- Automated testing frameworks\n\n**Developer Experience Features**:\n- One-click environment provisioning\n- Automated service discovery and registration\n- Built-in monitoring and alerting\n- Documentation generation from code\n\n**Platform Services**:\n- **Config Management**: Centralized configuration with Helm/Kustomize\n- **Secret Management**: Integration with HashiCorp Vault\n- **Service Mesh**: Istio for traffic management and security\n- **Database as a Service**: Managed database provisioning\n\n**Adoption Strategy**:\n1. Start with 2-3 pilot teams\n2. Implement feedback loops\n3. Gradual rollout with training\n4. Measure developer productivity metrics\n\n**Success Metrics**:\n- Time to production (target: <1 day)\n- Developer satisfaction score\n- Deployment frequency increase\n- Mean time to recovery improvement", "domain": "platform"}
{"instruction": "Troubleshoot Kubernetes pod crash loop", "input": "Pod in CrashLoopBackOff state, logs show 'OOMKilled' repeatedly, memory limit set to 512Mi, application is Java Spring Boot", "output": "**Root Cause**: Java application exceeding memory limits, likely due to JVM heap sizing issues.\n\n**Analysis**:\n- Spring Boot default heap size can consume most of container memory\n- JVM doesn't respect container limits without proper configuration\n- No memory reserved for non-heap usage (metaspace, direct memory, OS)\n\n**Immediate Fix**:\n1. Increase memory limit to 1Gi temporarily\n2. Add JVM arguments to respect container limits:\n   ```yaml\n   env:\n   - name: JAVA_OPTS\n     value: \"-XX:+UseContainerSupport -XX:MaxRAMPercentage=75.0\"\n   ```\n\n**Long-term Solutions**:\n1. **Right-size resources**:\n   - Set requests: 512Mi, limits: 1Gi\n   - Monitor actual usage with Prometheus\n\n2. **JVM Optimization**:\n   - Use G1GC for better memory management\n   - Enable JVM memory monitoring\n   - Consider using JVM memory calculator\n\n3. **Application Optimization**:\n   - Profile application for memory leaks\n   - Optimize Spring Boot configuration\n   - Consider reactive programming model\n\n**Monitoring**:\n- Add memory usage alerts\n- Implement JVM metrics collection\n- Set up application performance monitoring", "domain": "devops"}
{"instruction": "Analyze security incident in cloud environment", "input": "AWS CloudTrail shows unusual API calls: multiple IAM role assumptions from unfamiliar IP addresses, S3 bucket policy changes, and EC2 instances launched in new regions", "output": "**Security Incident Analysis - Potential Account Compromise**\n\n**Threat Classification**: High Severity - Potential account takeover\n\n**Attack Pattern Analysis**:\n1. **Lateral Movement**: IAM role assumptions suggest privilege escalation\n2. **Data Access**: S3 bucket policy changes indicate potential data exfiltration\n3. **Resource Abuse**: EC2 instances in new regions suggest cryptomining or botnet activity\n\n**Immediate Response Actions**:\n1. **Containment**:\n   - Rotate all IAM access keys immediately\n   - Disable compromised IAM users/roles\n   - Block suspicious IP addresses in security groups\n   - Terminate unauthorized EC2 instances\n\n2. **Investigation**:\n   - Analyze CloudTrail logs for full attack timeline\n   - Check S3 access logs for data access patterns\n   - Review VPC Flow Logs for network activity\n   - Examine EC2 instances for malware/mining software\n\n**Forensic Analysis**:\n- Preserve CloudTrail logs and snapshots\n- Document all suspicious activities\n- Check for persistence mechanisms (backdoors, scheduled tasks)\n- Analyze network traffic patterns\n\n**Recovery Steps**:\n1. Implement least privilege access\n2. Enable MFA for all IAM users\n3. Set up CloudWatch alerts for unusual activities\n4. Implement AWS Config for compliance monitoring\n5. Review and update incident response procedures\n\n**Prevention Measures**:\n- Enable GuardDuty for threat detection\n- Implement AWS Organizations SCPs\n- Regular security audits and penetration testing\n- Security awareness training for team", "domain": "sre"}
{"instruction": "Optimize cloud costs for over-provisioned infrastructure", "input": "Monthly AWS bill: $50,000. Analysis shows: 60% CPU utilization on average, many idle RDS instances, large EBS volumes with low usage, data transfer costs high", "output": "**Cloud Cost Optimization Analysis**\n\n**Current State**: $50,000/month with significant over-provisioning\n**Target**: 30-40% cost reduction ($15,000-$20,000 savings)\n\n**Optimization Opportunities**:\n\n**1. Compute Optimization (Est. savings: $8,000/month)**\n- Right-size EC2 instances based on 60% average utilization\n- Implement auto-scaling to handle peak loads\n- Use Spot Instances for non-critical workloads (70% savings)\n- Reserved Instances for predictable workloads (30-50% savings)\n\n**2. Database Optimization (Est. savings: $5,000/month)**\n- Consolidate idle RDS instances\n- Use Aurora Serverless for variable workloads\n- Implement read replicas only where needed\n- Schedule non-production databases to run only during business hours\n\n**3. Storage Optimization (Est. savings: $3,000/month)**\n- Implement S3 lifecycle policies (IA, Glacier transitions)\n- Clean up unused EBS snapshots and volumes\n- Use gp3 instead of gp2 for better price/performance\n- Implement EBS volume right-sizing\n\n**4. Network Optimization (Est. savings: $2,000/month)**\n- Use CloudFront CDN to reduce data transfer\n- Optimize inter-region traffic patterns\n- Implement VPC endpoints for AWS services\n- Review and optimize NAT Gateway usage\n\n**Implementation Plan**:\n1. **Week 1-2**: Implement monitoring and cost allocation tags\n2. **Week 3-4**: Right-size compute resources\n3. **Month 2**: Optimize storage and databases\n4. **Month 3**: Implement Reserved Instances and Savings Plans\n\n**Governance**:\n- Implement cost budgets and alerts\n- Regular cost review meetings\n- FinOps practices and training\n- Automated cost optimization tools", "domain": "cloud"}
{"instruction": "Design disaster recovery plan for critical e-commerce platform", "input": "E-commerce platform: 1M+ daily users, $10M monthly revenue, current RTO: 8 hours, RPO: 2 hours. Requirements: RTO < 1 hour, RPO < 15 minutes", "output": "**Disaster Recovery Plan - E-commerce Platform**\n\n**Current vs Target**:\n- RTO: 8h → <1h (87% improvement)\n- RPO: 2h → <15min (87% improvement)\n- Revenue at risk: $13,889/hour → $1,736/hour\n\n**DR Strategy: Multi-Region Active-Passive**\n\n**Architecture Design**:\n1. **Primary Region**: Production workloads (us-east-1)\n2. **DR Region**: Warm standby (us-west-2)\n3. **Database**: Cross-region replication with <5min lag\n4. **Storage**: Real-time S3 cross-region replication\n5. **CDN**: Global CloudFront distribution\n\n**Implementation Components**:\n\n**1. Database Replication**:\n- Aurora Global Database for <1 second replication lag\n- Read replicas in DR region for immediate failover\n- Automated backup to S3 with point-in-time recovery\n\n**2. Application Layer**:\n- Blue-green deployment across regions\n- Container images replicated to DR region ECR\n- Auto-scaling groups pre-configured in DR region\n\n**3. Data Synchronization**:\n- Real-time S3 cross-region replication\n- ElastiCache Global Datastore for session data\n- Search indices replicated to DR region\n\n**4. Network & DNS**:\n- Route 53 health checks with automatic failover\n- Load balancers pre-configured in DR region\n- VPC peering for secure communication\n\n**Failover Procedures**:\n1. **Automated Detection**: Health checks trigger within 2 minutes\n2. **DNS Failover**: Route 53 switches traffic (2-3 minutes)\n3. **Database Promotion**: Aurora promotes read replica (1-2 minutes)\n4. **Application Scaling**: Auto-scaling brings up instances (3-5 minutes)\n5. **Validation**: Automated testing confirms functionality (2-3 minutes)\n\n**Total Failover Time**: 10-15 minutes (well within 1-hour RTO)\n\n**Testing Strategy**:\n- Monthly DR drills with partial failover\n- Quarterly full DR exercises\n- Automated testing of DR procedures\n- Game day exercises with full team\n\n**Cost Analysis**:\n- DR infrastructure: $15,000/month (warm standby)\n- Replication costs: $5,000/month\n- Total DR cost: $20,000/month\n- ROI: Prevents $13,889/hour revenue loss\n\n**Monitoring & Alerting**:\n- Cross-region monitoring with Datadog/New Relic\n- RTO/RPO SLA monitoring\n- Automated runbook execution\n- 24/7 on-call rotation for DR events", "domain": "sre"}
{"instruction": "Implement observability strategy for microservices", "input": "15 microservices, Kubernetes cluster, current monitoring: basic Prometheus metrics, developers complain about difficulty troubleshooting issues across services", "output": "**Comprehensive Observability Strategy**\n\n**Three Pillars Implementation**:\n\n**1. Metrics (Enhanced)**\n- **Application Metrics**: RED method (Rate, Errors, Duration)\n- **Infrastructure Metrics**: USE method (Utilization, Saturation, Errors)\n- **Business Metrics**: Custom metrics for business KPIs\n- **SLI Metrics**: Service Level Indicators for each service\n\n**2. Logging (Structured)**\n- **Centralized Logging**: ELK/EFK stack with Fluentd\n- **Structured Logs**: JSON format with correlation IDs\n- **Log Levels**: Standardized across all services\n- **Security**: Log sanitization and retention policies\n\n**3. Tracing (Distributed)**\n- **Jaeger/Zipkin**: End-to-end request tracing\n- **OpenTelemetry**: Vendor-neutral instrumentation\n- **Sampling Strategy**: Adaptive sampling for performance\n- **Context Propagation**: Trace context across service boundaries\n\n**Implementation Plan**:\n\n**Phase 1: Foundation (Weeks 1-4)**\n- Deploy Jaeger for distributed tracing\n- Implement OpenTelemetry SDK in all services\n- Standardize logging format and correlation IDs\n- Set up centralized logging with ELK stack\n\n**Phase 2: Enhancement (Weeks 5-8)**\n- Create service dependency maps\n- Implement SLI/SLO monitoring\n- Set up alerting based on SLOs\n- Create observability dashboards\n\n**Phase 3: Optimization (Weeks 9-12)**\n- Implement automated anomaly detection\n- Create runbooks linked to alerts\n- Set up chaos engineering experiments\n- Implement observability as code\n\n**Developer Experience**:\n- **Unified Dashboard**: Single pane of glass (Grafana)\n- **Correlation**: Link metrics, logs, and traces\n- **Search**: Full-text search across all telemetry\n- **Alerting**: Context-rich alerts with remediation links\n\n**Key Metrics to Track**:\n- **Golden Signals**: Latency, Traffic, Errors, Saturation\n- **Service Map**: Real-time dependency visualization\n- **Error Budget**: SLO compliance tracking\n- **MTTR**: Mean time to resolution\n\n**Tooling Stack**:\n- **Metrics**: Prometheus + Grafana\n- **Logging**: ELK Stack (Elasticsearch, Logstash, Kibana)\n- **Tracing**: Jaeger with OpenTelemetry\n- **APM**: Consider Datadog/New Relic for enhanced features\n\n**Success Criteria**:\n- Reduce MTTR by 50%\n- 100% service coverage for observability\n- Developer satisfaction score >4/5\n- Zero blind spots in service interactions", "domain": "platform"}
{"instruction": "Analyze performance bottleneck in distributed system", "input": "API response times increased from 200ms to 2000ms over past week. System has: API Gateway, 5 microservices, Redis cache, PostgreSQL database, Kafka message queue. CPU usage normal, memory usage slightly elevated", "output": "**Performance Bottleneck Analysis**\n\n**Symptoms Analysis**:\n- 10x increase in response time (200ms → 2000ms)\n- Normal CPU, slightly elevated memory\n- Distributed system with multiple components\n\n**Systematic Investigation Approach**:\n\n**1. Request Flow Analysis**\n```\nClient → API Gateway → Microservice A → Cache/DB → Response\n```\n\n**2. Component-Level Investigation**:\n\n**API Gateway**:\n- Check connection pool exhaustion\n- Review rate limiting and throttling\n- Analyze request routing latency\n\n**Microservices**:\n- Examine service-to-service communication\n- Check for thread pool exhaustion\n- Review garbage collection metrics (if JVM)\n\n**Redis Cache**:\n- Monitor cache hit ratio (should be >90%)\n- Check for memory pressure and evictions\n- Analyze slow log for expensive operations\n\n**PostgreSQL Database**:\n- Review slow query log\n- Check for lock contention\n- Analyze connection pool utilization\n\n**Kafka Queue**:\n- Monitor consumer lag\n- Check for partition hotspots\n- Review producer/consumer throughput\n\n**Likely Root Causes (Priority Order)**:\n\n**1. Database Performance (High Probability)**\n- Slow queries due to missing indexes\n- Lock contention from long-running transactions\n- Connection pool exhaustion\n- Statistics out of date causing poor query plans\n\n**2. Cache Issues (Medium Probability)**\n- Cache invalidation storm\n- Redis memory pressure causing evictions\n- Network latency to cache cluster\n\n**3. Service Dependencies (Medium Probability)**\n- Downstream service degradation\n- Circuit breaker not configured\n- Retry storms amplifying issues\n\n**Diagnostic Commands**:\n```bash\n# Database analysis\nSELECT * FROM pg_stat_activity WHERE state = 'active';\nSELECT * FROM pg_locks WHERE NOT granted;\n\n# Redis analysis\nREDIS-CLI SLOWLOG GET 10\nREDIS-CLI INFO memory\n\n# Application metrics\nkubectl top pods\nkubectl logs -f deployment/api-service\n```\n\n**Immediate Actions**:\n1. Enable detailed application profiling\n2. Check database query performance\n3. Verify cache hit rates\n4. Review recent deployments/config changes\n\n**Long-term Solutions**:\n1. Implement distributed tracing\n2. Set up synthetic monitoring\n3. Create performance regression tests\n4. Implement circuit breakers and bulkheads", "domain": "sre"}
{"instruction": "Design CI/CD pipeline for microservices with security scanning", "input": "10 microservices in mono-repo, Docker containers, Kubernetes deployment, need automated security scanning, compliance with SOC2, deployment to staging and production environments", "output": "**Secure CI/CD Pipeline Design**\n\n**Pipeline Architecture**:\n```\nCode → Security Scan → Build → Test → Security Scan → Deploy → Monitor\n```\n\n**Multi-Stage Pipeline Implementation**:\n\n**Stage 1: Code Quality & Security**\n- **SAST (Static Analysis)**: SonarQube, Checkmarx\n- **Secret Scanning**: GitLeaks, TruffleHog\n- **Dependency Scanning**: Snyk, OWASP Dependency Check\n- **Code Quality**: ESLint, Prettier, unit tests\n\n**Stage 2: Container Security**\n- **Image Scanning**: Trivy, Clair, Aqua Security\n- **Dockerfile Linting**: Hadolint\n- **Base Image Validation**: Only approved base images\n- **Vulnerability Assessment**: CVE scanning\n\n**Stage 3: Deployment Security**\n- **Kubernetes Manifests**: Polaris, OPA Gatekeeper\n- **Network Policies**: Automated security policy validation\n- **RBAC Validation**: Least privilege access checks\n- **Secrets Management**: HashiCorp Vault integration\n\n**Pipeline Configuration (GitLab CI/GitHub Actions)**:\n\n```yaml\nstages:\n  - security-scan\n  - build\n  - test\n  - security-validation\n  - deploy-staging\n  - integration-tests\n  - deploy-production\n\nsecurity-scan:\n  stage: security-scan\n  script:\n    - sonar-scanner\n    - gitleaks detect\n    - snyk test\n  artifacts:\n    reports:\n      security: security-report.json\n\nbuild:\n  stage: build\n  script:\n    - docker build -t $IMAGE_TAG .\n    - trivy image $IMAGE_TAG\n    - docker push $IMAGE_TAG\n  only:\n    - main\n    - develop\n\ndeploy-staging:\n  stage: deploy-staging\n  script:\n    - helm upgrade --install app ./charts/app\n    - kubectl apply -f k8s/network-policies.yaml\n  environment:\n    name: staging\n  only:\n    - develop\n\ndeploy-production:\n  stage: deploy-production\n  script:\n    - helm upgrade --install app ./charts/app\n    - kubectl apply -f k8s/production/\n  environment:\n    name: production\n  when: manual\n  only:\n    - main\n```\n\n**Security Controls**:\n\n**1. SOC2 Compliance**:\n- Audit logging for all pipeline activities\n- Access controls with MFA\n- Encryption at rest and in transit\n- Regular security assessments\n\n**2. Shift-Left Security**:\n- Pre-commit hooks for basic security checks\n- IDE plugins for real-time vulnerability detection\n- Developer security training\n\n**3. Runtime Security**:\n- Falco for runtime threat detection\n- Network policies for micro-segmentation\n- Pod security policies/standards\n- Regular penetration testing\n\n**Quality Gates**:\n- Security scan results must pass\n- Code coverage >80%\n- No high/critical vulnerabilities\n- Performance tests within SLA\n\n**Deployment Strategies**:\n- **Staging**: Blue-green deployment\n- **Production**: Canary deployment with automated rollback\n- **Feature flags**: Gradual feature rollout\n- **Database migrations**: Zero-downtime migrations\n\n**Monitoring & Observability**:\n- Pipeline metrics and alerts\n- Deployment success/failure tracking\n- Security metrics dashboard\n- DORA metrics (deployment frequency, lead time, MTTR, change failure rate)\n\n**Tool Stack**:\n- **CI/CD**: GitLab CI or GitHub Actions\n- **Security**: SonarQube, Snyk, Trivy\n- **Container Registry**: Harbor with security scanning\n- **Secrets**: HashiCorp Vault\n- **Monitoring**: Prometheus, Grafana\n- **Compliance**: Compliance as Code with OPA\n\n**Success Metrics**:\n- Zero security vulnerabilities in production\n- <30 minute deployment time\n- 99.9% deployment success rate\n- SOC2 audit compliance", "domain": "devops"}
